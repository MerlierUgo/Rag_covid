{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from langchain_ollama import OllamaLLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charger les fichiers CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_1 = pd.read_csv('/home/cytech/test/Rag_covid/data/data_preprocessing/Age_Preprocessing.csv')\n",
    "data_2 = pd.read_csv('/home/cytech/test/Rag_covid/data/data_preprocessing/Diabetes_Preprocessing.csv')\n",
    "data_3 = pd.read_csv('/home/cytech/test/Rag_covid/data/data_preprocessing/Overweight_or_obese_Preprocessing.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fusionner les donn√©es\n",
    "combined_data = pd.concat([data_1, data_2, data_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction segmenter_texte et build_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour segmenter un document en morceaux de 512 tokens\n",
    "def segmenter_texte(texte, longueur_max=512):\n",
    "    tokens = texte.split()\n",
    "    segments = []\n",
    "    for i in range(0, len(tokens), longueur_max):\n",
    "        segment = ' '.join(tokens[i:i + longueur_max])\n",
    "        segments.append(segment)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre instruction par d√©faut\n",
    "default_instruction = (\"You're an empathetic doctor who knows how to synthesize things so that patients simply understand. You have access to numerous scientific journals (data in the form of embedding). A patient, frightened about his current disease, covid 19, asks you a question about a factor he thinks may or may not be at risk. You have to give him a summary answer, based on the abstracts you have from scientific journals. Simply explain whether or not this factor is a risk in terms of the severity of the virus or its lethality. Don't hesitate to be understanding and gentle. Patients can be stressed and worried. Here's the patient's\")\n",
    "\n",
    "# Fonction pour construire la requ√™te compl√®te en ajoutant l'instruction par d√©faut\n",
    "def build_query(user_query):\n",
    "    return default_instruction + \"\\n\" + user_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la segmentation √† chaque document de la colonne 'context'\n",
    "documents = combined_data['context'].dropna().tolist()\n",
    "segmented_docs = []\n",
    "for doc in documents:\n",
    "    segmented_docs.extend(segmenter_texte(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialiser le mod√®le d'embedding avec une dimension r√©duite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_144466/4237313238.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "/home/cytech/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/static-retrieval-mrl-en-v1\",\n",
    "    model_kwargs={'device': 'cpu', 'truncate_dim': 1024}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cr√©er l‚Äôindex Chroma dans un nouveau dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_dir = \"./chroma_static_mrl\"\n",
    "vectorstore = Chroma.from_texts(segmented_docs, embedding_model, persist_directory=persist_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le syst√®me de r√©cup√©ration\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialiser le mod√®le LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_144466/2615984379.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:3b\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.2:3b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire la cha√Æne QA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de requ√™te utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so glad you asked, and I want to assure you that we're going to take a look at some research together to understand more about this topic.\n",
      "\n",
      "From what I've found in scientific journals, being overweight or obese has been linked to a higher risk of severe illness from COVID-19. Studies have shown that people with a body mass index (BMI) of 30 or higher are more likely to develop pneumonia and acute respiratory distress syndrome (ARDS), which can be serious complications.\n",
      "\n",
      "In terms of mortality rates, research suggests that people who are overweight or obese may also be at a slightly increased risk of dying from COVID-19. However, it's essential to note that the overall risk is still relatively low, even for those with higher BMIs.\n",
      "\n",
      "A study published in the New England Journal of Medicine found that among patients hospitalized with COVID-19, those who were obese had a 1.8 times higher risk of death compared to those with a normal weight.\n",
      "\n",
      "Another study published in the International Journal of Obesity found that people with a BMI of 30 or higher had a 25% increased risk of developing severe illness from COVID-19.\n",
      "\n",
      "Now, I want to emphasize that these findings are based on averages and should not be taken as a guarantee for individual outcomes. Everyone's body is different, and many factors can influence the severity of COVID-19.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Give me figures on covid and overweight\"\n",
    "\n",
    "# Construction de la requ√™te compl√®te\n",
    "query = build_query(user_query)\n",
    "\n",
    "# Ex√©cution de la cha√Æne QA avec la requ√™te compl√®te\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "print(\"üìù Docs r√©cup√©r√©s :\")\n",
    "for d in docs:\n",
    "    print(d.page_content[:300])  # Affiche les premiers caract√®res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Partie interface de chat avec historique ---\n",
    "# Cr√©ation de la fen√™tre principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.title(\"Chat avec LLM\")\n",
    "\n",
    "# Widget Text pour afficher la conversation\n",
    "output_text = tk.Text(root, wrap=tk.WORD, height=20, width=80)\n",
    "output_text.pack(padx=10, pady=10)\n",
    "\n",
    "# Widget Entry pour saisir le message de l'utilisateur\n",
    "entry = tk.Entry(root, width=80)\n",
    "entry.pack(padx=10, pady=(0,10))\n",
    "\n",
    "# Instruction par d√©faut √† ajouter √† chaque requ√™te\n",
    "default_instruction = (\"You're an empathetic doctor who knows how to synthesize things so that patients simply understand. You have access to numerous scientific journals (data in the form of embedding). A patient, frightened about his current disease, covid 19, asks you a question about a factor he thinks may or may not be at risk. You have to give him a summary answer, based on the abstracts you have from scientific journals. Simply explain whether or not this factor is a risk in terms of the severity of the virus or its lethality. Don't hesitate to be understanding and gentle. Patients can be stressed and worried. Here's the patient\")\n",
    "\n",
    "# Initialiser l'historique du chat avec le message syst√®me\n",
    "chat_history = [{\"role\": \"system\", \"content\": default_instruction}]\n",
    "\n",
    "# Pour le chat, on peut utiliser le m√™me mod√®le (ou en instancier un nouveau)\n",
    "llm_chat = llm\n",
    "\n",
    "def get_ai_response():\n",
    "    \"\"\"R√©cup√®re la r√©ponse du mod√®le et garde l'historique.\"\"\"\n",
    "    user_input = entry.get().strip()\n",
    "    if not user_input:\n",
    "        output_text.insert(tk.END, \"Veuillez entrer un message.\\n\")\n",
    "        return\n",
    "\n",
    "    # Ajouter l'entr√©e utilisateur √† l'historique\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Construire le prompt format√© en int√©grant l'historique complet\n",
    "    formatted_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "\n",
    "    # Obtenir la r√©ponse du mod√®le\n",
    "    response = llm_chat.invoke(formatted_prompt)\n",
    "\n",
    "    # Ajouter la r√©ponse de l'assistant √† l'historique\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    # Afficher la conversation dans le widget de sortie\n",
    "    output_text.insert(tk.END, f\"Utilisateur : {user_input}\\nIA : {response}\\n\\n\")\n",
    "\n",
    "    # Effacer le champ de saisie\n",
    "    entry.delete(0, tk.END)\n",
    "\n",
    "# Bouton pour envoyer le message\n",
    "send_button = tk.Button(root, text=\"Envoyer\", command=get_ai_response)\n",
    "send_button.pack(padx=10, pady=(0,10))\n",
    "\n",
    "# Lancer la boucle principale de l'interface Tkinter\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
