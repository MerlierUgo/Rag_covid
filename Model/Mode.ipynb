{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from langchain_ollama import OllamaLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les fichiers CSV\n",
    "data_1 = pd.read_csv('/home/cytech/test/Rag_covid/data/data_preprocessing/Age_Preprocessing.csv')\n",
    "data_2 = pd.read_csv('/home/cytech/test/Rag_covid/data/data_preprocessing/Diabetes_Preprocessing.csv')\n",
    "data_3 = pd.read_csv('/home/cytech/test/Rag_covid/data/data_preprocessing/Overweight_or_obese_Preprocessing.csv')\n",
    "\n",
    "# Concaténer les datasets si nécessaire\n",
    "dataset = pd.concat([data_1, data_2, data_3], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs manquantes par des chaînes vides et convertir en type str\n",
    "dataset['context'] = dataset['context'].fillna('').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que la colonne 'context' contient le texte pertinent\n",
    "documents = dataset['context'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le modèle d'embedding\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Créer l'index Chroma\n",
    "vectorstore = Chroma.from_texts(documents, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2:3b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_537259/1745571089.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je n'ai pas trouvé d'étude ou de données qui mentionnent l'efficacité du vaccin contre le COVID-19 dans ces sources. Les études présentées sont toutes des observations retrospectives sur les patients hospitalisés pour COVID-19, mais elles ne traitent pas explicitement la question de l'efficacité des vaccinations contre cette maladie.\n",
      "\n",
      "Si vous recherchez des informations sur l'efficacité du vaccin contre le COVID-19, je vous recommande de consulter les sources les plus récentes et les plus fiables, telles que les publications scientifiques dans des revues médicales ou les rapports officiels de santé publique.\n"
     ]
    }
   ],
   "source": [
    "query = \"Quelle est l'efficacité du vaccin contre le COVID-19 ?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided studies, here are some figures related to COVID-19 and being overweight:\n",
      "\n",
      "1. Obesity as a risk factor for Covid-19 hospital admission:\n",
      "\t* Age < 60 years: OR 1.8 (Severe), with a significant p-value (<0.01)\n",
      "\t* Age > 60 years: OR 1.1 (not significant, with a p-value of 0.57)\n",
      "2. Obesity and COVID-19 mortality risk:\n",
      "\t* Age < 60 years: Fatality OR not calculated\n",
      "\t* Age > 60 years: Fatality OR 2.06 (not significant, with a p-value of 0.55), but fatality range is quite broad (0.19-22.44) and the confidence interval suggests that there may be an association, although it's not statistically significant at current levels.\n",
      "\n",
      "It's worth noting that these studies are observational in nature, so while they provide some insight into the relationship between obesity and COVID-19 outcomes, they cannot establish causality. Additionally, the sample sizes are relatively small compared to larger studies or meta-analyses.\n",
      "\n",
      "If you're looking for more information or want to explore further, I recommend searching for more recent or comprehensive studies on this topic.\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me figures on covid and overweight\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def get_ai_response():\n",
    "    \"\"\"Récupère la réponse du modèle et met à jour l'historique.\"\"\"\n",
    "    user_input = entry.get().strip()\n",
    "    if not user_input:\n",
    "        output_text.insert(tk.END, \"Veuillez entrer un message.\\n\")\n",
    "        return\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    formatted_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    output_text.insert(tk.END, f\"Utilisateur : {user_input}\\nIA : {response}\\n\\n\")\n",
    "\n",
    "    entry.delete(0, tk.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la fenêtre principale\n",
    "root = tk.Tk()\n",
    "root.title(\"Chat avec Llama3\")\n",
    "root.geometry(\"600x400\")\n",
    "\n",
    "# Champ de saisie\n",
    "entry = tk.Entry(root, width=60)\n",
    "entry.pack(pady=10)\n",
    "\n",
    "# Bouton d'envoi\n",
    "send_button = tk.Button(root, text=\"Envoyer\", command=get_ai_response)\n",
    "send_button.pack()\n",
    "\n",
    "# Zone de texte pour afficher la conversation\n",
    "output_text = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=70, height=15)\n",
    "output_text.pack(pady=10)\n",
    "\n",
    "# Lancement de la boucle principale de l'interface\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
