


# Versatile Diffusion Model


![Exemple de sortie](./assets/front.png)


## Introduction
<p align="justify">
L'intelligence artificielle générative a connu un essor spectaculaire ces dernières années, notamment avec les progrès réalisés dans les modèles capables de générer des images, du texte, de la musique et d'autres types de contenus à partir de données d'entrée. Parmi ces modèles, les modèles de diffusion ont émergé comme l'une des approches les plus prometteuses pour la génération de contenu visuel. Ces modèles ont prouvé leur efficacité dans la création d'images réalistes à partir de bruit aléatoire en inversant progressivement un processus de diffusion. Cela permet de générer des images de haute qualité à partir de descriptions textuelles ou de bruit aléatoire.
Les modèles de diffusion classiques, tels que Stable Diffusion ou DALL-E, sont principalement spécialisés dans une tâche donnée, soit la génération d'images à partir de texte ou la création d'images réalistes à partir de bruit. Cependant, ces modèles, bien que puissants, sont souvent limités dans leur capacité à traiter plusieurs modalités de manière fluide. Cela signifie qu'ils ne peuvent pas simultanément gérer plusieurs types de données, comme des images et des textes, dans une approche flexible. C'est ici qu'intervient le modèle Versatile Diffusion (VD), qui propose une approche intégrée et flexible pour gérer à la fois des images et du texte dans un seul modèle.
</p>



<p align="center">
  <img src="./assets/model.png" />
</p>

<p align="center">
  Figure 1 Schéma du modèle versatile utilisé dans la revue
</p>
<br>
