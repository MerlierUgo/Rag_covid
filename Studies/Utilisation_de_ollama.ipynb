{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2:3b\")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def get_ai_response():\n",
    "    \"\"\"Récupère la réponse du modèle et garde l'historique.\"\"\"\n",
    "    user_input = entry.get().strip()\n",
    "    if not user_input:\n",
    "        output_text.insert(tk.END, \"Veuillez entrer un message.\\n\")\n",
    "        return\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    formatted_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    output_text.insert(tk.END, f\"Utilisateur : {user_input}\\nIA : {response}\\n\\n\")\n",
    "\n",
    "    entry.delete(0, tk.END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_backends\\sync.py\", line 215, in connect_tcp\n",
      "    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
      "  File \"C:\\Users\\ugome\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ugome\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\ugome\\AppData\\Local\\Temp\\ipykernel_22416\\457653764.py\", line 16, in get_ai_response\n",
      "    response = llm.invoke(formatted_prompt)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 390, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 763, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 966, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 787, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_ollama\\llms.py\", line 288, in _generate\n",
      "    final_chunk = self._stream_with_aggregation(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_ollama\\llms.py\", line 256, in _stream_with_aggregation\n",
      "    for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\langchain_ollama\\llms.py\", line 211, in _create_generate_stream\n",
      "    yield from self._client.generate(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\ollama\\_client.py\", line 163, in inner\n",
      "    with self._client.stream(*args, **kwargs) as r:\n",
      "  File \"C:\\Users\\ugome\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 119, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_client.py\", line 868, in stream\n",
      "    response = self.send(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"C:\\Users\\ugome\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"d:\\01_Etudes\\ING3\\RAG\\env_rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée\n"
     ]
    }
   ],
   "source": [
    "root = tk.Tk()\n",
    "root.title(\"Chat avec Llama3\")\n",
    "root.geometry(\"600x400\")\n",
    "\n",
    "entry = tk.Entry(root, width=60)\n",
    "entry.pack(pady=10)\n",
    "\n",
    "send_button = tk.Button(root, text=\"Envoyer\", command=get_ai_response)\n",
    "send_button.pack()\n",
    "\n",
    "output_text = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=70, height=15)\n",
    "output_text.pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
